{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Module\n",
    "BY --<i>BHARAT SRI HARSHA KARPURAPU</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This is a general-purpose sentiment module, which I developed for calculating the sentiment in my other projects where this module is used as a library for giving sentiment. This module is well built with different classifiers like Naive Bayes, SVC, Bernoulli, Multinomial Naive Bayes and logistic Regression. The final sentiment result was given by the mode value of all the different classifiers. Confidence score was also given by this module by taking certain considerations.</p>\n",
    "<p>Kindly go through the entire notebook for detailed understanding. If any questions, please feel free to contact <b><i> kbsriharsha@gmail.com</i></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class with methods for giving mode of the classification list and for giving sentiment score\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return Counter(votes).most_common(1)[0][0]\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(Counter(votes).most_common(1)[0][0])\n",
    "        conf = float(choice_votes) / float(len(votes))\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To make the default encoding to utf-8, so as to make platform independent to tackle encoding problem.\n",
    "import sys  \n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performing natural language processing operation like tokenizing, stemming, chunking, etc.\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "def required_words(tweet):\n",
    "    stop_words_nltk = stopwords.words('english')\n",
    "    stop_words_sklearn = text.ENGLISH_STOP_WORDS\n",
    "    stop = set(stop_words_nltk) | set(stop_words_sklearn)\n",
    "    punc = set([\"#\",\".\",\"{\",\"}\",\"{}\",\"@\",\"\\\\\",\"/\",\",\",\"!\",\"?\",\"'\",\"|\"])\n",
    "    words = [re.sub(r\"[^\\x00-\\x7F]\",\"\",re.sub(r\"[\\-\\d\\\\./?'|]\",\"\",x.lower())) for x in word_tokenize(tweet) if x not in stop | punc]\n",
    "    words_replaced = filter(lambda a: a != \"\", words)\n",
    "    pos_words = nltk.pos_tag(words_replaced)\n",
    "    #print pos_words\n",
    "    adjectives = [x[0] for x in pos_words if x[1] in [\"JJ\",\"JJR\",\"JJS\"]]\n",
    "    nouns = [x[0] for x in pos_words if x[1] in [\"NN\",\"NNP\",\"NNPS\", \"NNS\"]]\n",
    "    verbs = [x[0] for x in pos_words if x[1] in [\"RBS\",\"RBR\",\"RB\",\"VB\"]]\n",
    "    #return adjectives+nouns\n",
    "    return \" \".join(adjectives+nouns+verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading the positive text for features\n",
    "req_tags = [\"JJ\",\"JJR\",\"JJS\",\"VB\",\"RB\",\"RBR\",\"RBS\"]\n",
    "documents = []\n",
    "all_words = []\n",
    "with open(\"positive.txt\",\"r\") as f:\n",
    "        positive_words = []\n",
    "        for line in f:\n",
    "            try:\n",
    "                lines = required_words(line)\n",
    "                documents.append( (lines, \"pos\") )\n",
    "                tokenize = word_tokenize(lines)\n",
    "                pos_tag = nltk.pos_tag(tokenize)\n",
    "                positive_words.append([x[0].lower() for x in pos_tag if x[1] in req_tags])    \n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading negative text for features\n",
    "with open(\"negative.txt\",\"r\") as f:\n",
    "        negative_words = []\n",
    "        for line in f:\n",
    "            try:\n",
    "                lines = required_words(line)\n",
    "                documents.append( (lines, \"neg\") )\n",
    "                tokenize = word_tokenize(lines)\n",
    "                neg_tag = nltk.pos_tag(tokenize)\n",
    "                negative_words.append([x[0].lower() for x in neg_tag if x[1] in req_tags])    \n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making a list of lists into single list\n",
    "positive_words = [x for y in positive_words for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14497"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making a list of lists to single list\n",
    "negative_words = [x for y in negative_words for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13728"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For calculationg the frequency distribution of positive words\n",
    "freq_pos = nltk.FreqDist(positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For calculating the propability distribution of the positive words\n",
    "prop_pos = [{freq_pos.keys()[x]:float(freq_pos.values()[x]/13431.00)} for x in range(len(freq_pos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For calculating the frequency distribution of negative words\n",
    "freq_neg = nltk.FreqDist(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For calculating the propability distribution of negative words\n",
    "prop_neg = [{freq_neg.keys()[x]:float(freq_neg.values()[x]/13431.00)} for x in range(len(freq_neg))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For unique feature words\n",
    "features_words = nltk.FreqDist(positive_words+negative_words).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6093"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving documents positive and negative documents\n",
    "document_save = open(\"documents.pickle\",\"wb\")\n",
    "pickle.dump(documents, document_save)\n",
    "document_save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving feature words\n",
    "features_save = open(\"features.pickle\",\"wb\")\n",
    "pickle.dump(\"features_words\", features_save)\n",
    "features_save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for constructing feature Vector\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in features_words:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constructing feature set for training and testing purpose\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10658"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10658"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(featuresets)\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = featuresets[:9000]\n",
    "testing_data = featuresets[9000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here different classifiers were trained and saved so that we can use at any point of time by just opeining the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "#base_classifier = nltk.NaiveBayesClassifier.train(training_data)\n",
    "#MNB_classifier = SklearnClassifier(MultinomialNB()).train(training_data)\n",
    "#BernoulliNB_classifier = SklearnClassifier(BernoulliNB()).train(training_data)\n",
    "#LinearSVC_classifier = SklearnClassifier(LinearSVC()).train(training_data)\n",
    "#LogisticRegression_classifier = SklearnClassifier(LogisticRegression()).train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save_classifier = open(\"algorithms_pickle/naive_bayes.pickle\",\"wb\")\n",
    "#pickle.dump(base_classifier, save_classifier)\n",
    "#save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save_classifier = open(\"algorithms_pickle/MNB.pickle\",\"wb\")\n",
    "#pickle.dump(MNB_classifier, save_classifier)\n",
    "#save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save_classifier = open(\"algorithms_pickle/Berno.pickle\",\"wb\")\n",
    "#pickle.dump(BernoulliNB_classifier, save_classifier)\n",
    "#save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save_classifier = open(\"algorithms_pickle/LineaSVC.pickle\",\"wb\")\n",
    "#pickle.dump(LinearSVC_classifier, save_classifier)\n",
    "#save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save_classifier = open(\"algorithms_pickle/Logistic.pickle\",\"wb\")\n",
    "#pickle.dump(LogisticRegression_classifier, save_classifier)\n",
    "#save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opening the trained Naive Bayes Classifier\n",
    "open_file = open(\"algorithms_pickle/naive_bayes.pickle\",\"rb\")\n",
    "Naive_Bayes_Classifier = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opeining trained Multinomial Naive Bayes Classifier\n",
    "open_file = open(\"algorithms_pickle/MNB.pickle\",\"rb\")\n",
    "MNB_Classifier = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opening the trained Bernoulli Classifier\n",
    "open_file = open(\"algorithms_pickle/Berno.pickle\",\"rb\")\n",
    "Berno_Classifier = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opening trained Linear SVC Classifier\n",
    "open_file = open(\"algorithms_pickle/LineaSVC.pickle\",\"rb\")\n",
    "Linear_SVC_Classifier = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opeining trained Logistic Regression Classifier\n",
    "open_file = open(\"algorithms_pickle/Logistic.pickle\",\"rb\")\n",
    "Logistic_Classifier = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building a Final classifier which will give the mode value of all the classifier results.\n",
    "Classifier_winner = VoteClassifier(Naive_Bayes_Classifier,\n",
    "                                   MNB_Classifier,\n",
    "                                   Berno_Classifier,\n",
    "                                   Linear_SVC_Classifier,\n",
    "                                   Logistic_Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a function which can be called from other program file to give the sentiment polarity\n",
    "def sentiment(tweet):\n",
    "    tweet_words = find_features(tweet)\n",
    "    return Classifier_winner.classify(tweet_words),Classifier_winner.confidence(tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 1.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"I am awesome classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Note</h4>\n",
    "<p>We can increase the performance by giving more training data. This module is being called by many of my other programs for evaluating sentiment sentiment polarity.</p>\n",
    "<p>For using in other programs, first import the module into your current program by <br><i>from Sentiment_Module import Sentiment</i> <br> and then pass the statement whose polarity needs to be determined through the function <br><i>Sentiment(<--statement-->)</i></p>\n",
    "\n",
    "<p>I calculated probability distribution of feature words but I didn't that one for training purpose. However, we can extend this model to more precise by giving propabilities of negativity and positivity using those propability distribution.</p>\n",
    "<p>I believe this module can be greatly developed futher with all the above ideas taken into consideration.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b> Topics: </b> Machine Learning, Natural Language Processing, Sentiment analysis</p>\n",
    "<p><b> Machine Learning Algorithms: </b> Naive Bayes classifier, Multinomial Naive Bayes classifier, Logistic Regression classifier, Linear SVC classifier, Bernoulli classifier</p>\n",
    "<p><b> Natural Language Processing:</b> Parsing, chunking, Parts of Speech tagging, Named Entity Recognition</p>\n",
    "<p><b> Compute: </b> python</p>\n",
    "<p><b> Data Tools:</b>Jupyter Notebook</p>\n",
    "<p><b> Version Control: </b> Git</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
