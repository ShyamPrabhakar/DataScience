{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science In Action - A Sentiment Analysis Framework\n",
    "BY --<i>BHARAT SRI HARSHA KARPURAPU</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second part in my theis. In this part, I used tweepy search api for using twitter Rest api. This part is more concentrated on developing a sentiment framework for single entity based on tweets collected using twitter search api. In this case, I used Regions Bank as the entity and collected the tweets. The system was programmed in such a case, when the sentiment score reached below threshold, the system will automatically send emails to corressponding people for taking actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import tweepy\n",
    "import socket\n",
    "import json\n",
    "import pandas as pd\n",
    "import Sentiment_Module as senti\n",
    "import codecs\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.MIMEBase import MIMEBase\n",
    "from email import Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "#import nltk\n",
    "#import random\n",
    "#from nltk.classify.scikitlearn import SklearnClassifier\n",
    "#import pickle\n",
    "#from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "#from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "#from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "#from nltk.classify import ClassifierI\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from scipy.stats import mode\n",
    "#from collections import Counter\n",
    "\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "#class VoteClassifier(ClassifierI):\n",
    "#    def __init__(self, *classifiers):\n",
    "#        self._classifiers = classifiers\n",
    "\n",
    "#    def classify(self, features):\n",
    "#        votes = []\n",
    "#        for c in self._classifiers:\n",
    "#            v = c.classify(features)\n",
    "#            votes.append(v)\n",
    "#        return Counter(votes).most_common(1)[0][0]\n",
    "\n",
    "#    def confidence(self, features):\n",
    "#        votes = []\n",
    "#        for c in self._classifiers:\n",
    "#            v = c.classify(features)\n",
    "#            votes.append(v)\n",
    "\n",
    "#        choice_votes = votes.count(Counter(votes).most_common(1)[0][0])\n",
    "#        conf = float(choice_votes) / float(len(votes))\n",
    "#        return conf\n",
    "\n",
    "\n",
    "# In[68]:\n",
    "\n",
    "#import sys  \n",
    "\n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('utf8')\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "#req_tags = [\"JJ\",\"JJR\",\"JJS\"]\n",
    "#documents = []\n",
    "#all_words = []\n",
    "#with open(\"positive.txt\",\"r\") as f:\n",
    "#        positive_words = []\n",
    "#        for line in f:\n",
    "#            try:\n",
    "#                documents.append( (line, \"pos\") )\n",
    "#              pos_tag = nltk.pos_tag(tokenize)\n",
    "#                positive_words.append([x[0].lower() for x in pos_tag if x[1] in req_tags])    \n",
    "#            except:\n",
    "#                continue\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "#with open(\"negative.txt\",\"r\") as f:\n",
    "#        negative_words = []\n",
    "#        for line in f:\n",
    "#            try:\n",
    "#                documents.append( (line, \"neg\") )\n",
    "#                tokenize = word_tokenize(line)\n",
    "#                neg_tag = nltk.pos_tag(tokenize)\n",
    "#                negative_words.append([x[0].lower() for x in neg_tag if x[1] in req_tags])    \n",
    "#            except:\n",
    "#                continue\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "#positive_words = [x for y in positive_words for x in y]\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "#negative_words = [x for y in negative_words for x in y]\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "#freq_pos = nltk.FreqDist(positive_words)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "#prop_pos = [{freq_pos.keys()[x]:float(freq_pos.values()[x]/13431.00)} for x in range(len(freq_pos))]\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "#freq_neg = nltk.FreqDist(negative_words)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "#prop_neg = [{freq_neg.keys()[x]:float(freq_neg.values()[x]/13431.00)} for x in range(len(freq_neg))]\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "#features_words = nltk.FreqDist(positive_words+negative_words).keys()\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "#document_save = open(\"documents.pickle\",\"wb\")\n",
    "#pickle.dump(documents, document_save)\n",
    "#document_save.close()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "#features_save = open(\"features.pickle\",\"wb\")\n",
    "#pickle.dump(\"features_words\", features_save)\n",
    "#features_save.close()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "#def find_features(document):\n",
    "#    words = word_tokenize(document)\n",
    "#    features = {}\n",
    "#    for w in features_words:\n",
    "#        features[w] = (w in words)\n",
    "#\n",
    "#    return features\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "#featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "\n",
    "# In[95]:\n",
    "\n",
    "#featuresets\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "#random.shuffle(featuresets)\n",
    "#len(featuresets)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "#training_data = featuresets[:9000]\n",
    "#testing_data = featuresets[9000:]\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "# Classifiers\n",
    "# base_classifier = nltk.NaiveBayesClassifier.train(training_data)\n",
    "# MNB_classifier = SklearnClassifier(MultinomialNB()).train(training_data)\n",
    "# BernoulliNB_classifier = SklearnClassifier(BernoulliNB()).train(training_data)\n",
    "# LinearSVC_classifier = SklearnClassifier(LinearSVC()).train(training_data)\n",
    "# LogisticRegression_classifier = SklearnClassifier(LogisticRegression()).train(training_data)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "# save_classifier = open(\"algorithms_pickle/naive_bayes.pickle\",\"wb\")\n",
    "# pickle.dump(base_classifier, save_classifier)\n",
    "# save_classifier.close()\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "# save_classifier = open(\"algorithms_pickle/MNB.pickle\",\"wb\")\n",
    "# pickle.dump(MNB_classifier, save_classifier)\n",
    "# save_classifier.close()\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "# save_classifier = open(\"algorithms_pickle/Berno.pickle\",\"wb\")\n",
    "# pickle.dump(BernoulliNB_classifier, save_classifier)\n",
    "# save_classifier.close()\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "# save_classifier = open(\"algorithms_pickle/LineaSVC.pickle\",\"wb\")\n",
    "# pickle.dump(LinearSVC_classifier, save_classifier)\n",
    "# save_classifier.close()\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "# save_classifier = open(\"algorithms_pickle/Logistic.pickle\",\"wb\")\n",
    "# pickle.dump(LogisticRegression_classifier, save_classifier)\n",
    "# save_classifier.close()\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "#open_file = open(\"algorithms_pickle/naive_bayes.pickle\",\"rb\")\n",
    "#Naive_Bayes_Classifier = pickle.load(open_file)\n",
    "#open_file.close()\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "#open_file = open(\"algorithms_pickle/MNB.pickle\",\"rb\")\n",
    "#MNB_Classifier = pickle.load(open_file)\n",
    "#open_file.close()\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "#open_file = open(\"algorithms_pickle/Berno.pickle\",\"rb\")\n",
    "#Berno_Classifier = pickle.load(open_file)\n",
    "#open_file.close()\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "#open_file = open(\"algorithms_pickle/LineaSVC.pickle\",\"rb\")\n",
    "#Linear_SVC_Classifier = pickle.load(open_file)\n",
    "#open_file.close()\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "#open_file = open(\"algorithms_pickle/Logistic.pickle\",\"rb\")\n",
    "#Logistic_Classifier = pickle.load(open_file)\n",
    "#open_file.close()\n",
    "\n",
    "\n",
    "# In[69]:\n",
    "\n",
    "#Classifier_winner = VoteClassifier(Naive_Bayes_Classifier,\n",
    " #                                  MNB_Classifier,\n",
    "#                                 Berno_Classifier,\n",
    "#                                   Linear_SVC_Classifier,\n",
    "#                                   Logistic_Classifier)\n",
    "#\n",
    "\n",
    "# In[70]:\n",
    "\n",
    "#def sentiment(tweet):\n",
    "#    tweet_words = find_features(tweet)\n",
    " #   return Classifier_winner.classify(tweet_words),Classifier_winner.confidence(tweet_words)\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "#sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\")\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Twitter Credentials\n",
    "CONSUMER_KEY = \"sNKyWnthaapoophzjUa1GV42R\"\n",
    "CONSUMER_SECRET = \"n0F14rS3XwXlg1q3H75jvjEBLpb86DtHbag6BSM54hqh4PQKGS\"\n",
    "OAUTH_TOKEN = \"610909012-xpo0HfBPwAqg2LPwvDnjVP73sEcIglO1LDABNFcI\"\n",
    "OAUTH_TOKEN_SECRET = \"y1WIBNEXGzL2GnkeOJ9mvROvAK6sjk1IB4nRy8lAbaCeP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Twitter Authentication\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Api Initiation\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a Data Frame For Storing Tweets\n",
    "Tweets_Frame = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For getting tweets on askRegions tag\n",
    "for tweet in tweepy.Cursor(api.search,q='askRegions').items():\n",
    "    print tweet.text\n",
    "    msg = tweet.text.encode('UTF-8') \n",
    "    file_tweets = open(\"tweets_sentiment_final.csv\",\"a\")\n",
    "    if senti.sentiment(tweet.text):\n",
    "        file_tweets.write(tweet.text)\n",
    "        file_tweets.write(\"\\n\")\n",
    "        file_tweets.close()\n",
    "    else:\n",
    "        continue\n",
    "    print msg\n",
    "    senti.sentiment(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"tweets_sentiment_final.csv\",\"r\",\"utf-8\") as f:\n",
    "    tweets_polarity = []\n",
    "    for line in f:     \n",
    "        tweets_polarity.append((line,senti.sentiment(line)[0],senti.sentiment(line)[1]))\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DataFrame = pd.DataFrame(tweets_polarity,columns=[\"tweets\",\"polarity\",\"vote_ratio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4> DataScience In Action </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through automation we can perform actions much faster. Here the idea is that, program itself identifies the alarming condition and sends an immediate email to the corresponding branch manager to take an immediate action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = DataFrame.groupby('polarity').polarity.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if counts[0]>counts[1]: # If negative tweets are greater than positive tweets\n",
    "    # Connecting to the google server\n",
    "    msg = MIMEMultipart()\n",
    "    \n",
    "    msg['From'] = 'sentimentbdarl@gmail.com'\n",
    "    \n",
    "    msg['To'] = 'kbsriharsha@gmail.com'\n",
    "    \n",
    "    msg['Subject'] = \"Alarming Condition\"\n",
    "    \n",
    "    part = MIMEBase('application', \"octet-stream\")\n",
    "    \n",
    "    part.set_payload(open(\"neg_tweets_sample.txt\", \"rb\").read())\n",
    "    \n",
    "    Encoders.encode_base64(part)\n",
    "    \n",
    "    part.add_header('Content-Disposition', 'attachment; filename=\"neg_tweets_sample.txt\"')\n",
    "    \n",
    "    msg.attach(part)\n",
    "    \n",
    "    mail_server = smtplib.SMTP(\"smtp.gmail.com:587\")\n",
    "    \n",
    "    #Identifying ourselves\n",
    "    mail_server.ehlo\n",
    "    \n",
    "    mail_server.starttls()\n",
    "     \n",
    "    #Logging in with my credentials\n",
    "    mail_server.login(\"sentimentbdarl@gmail.com\", \"bdarlsentiment\")\n",
    "    \n",
    "    mail_server.sendmail(\"sentimentbdarl@gmail.com\",\"kbsriharsha@gmail.com\",msg.as_string())\n",
    "    \n",
    "    mail_server.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Summary:</h4> \n",
    "<p>This framework was developed for doing sentiment analysis for only single entity. Here Regions was used as single sentity. The Final output has only 40% postive tweets and most of them are negative which are mainly on the customer service. So we can recommend to Regions, to improve their customer service for more customer satisfaction. The most important part of this framework is the system will automatically send personal emails to appropriate persons for speed action.</p>\n",
    "<p>This framework has a lot of future scope, we can perform per state sentiment analysis, per person, which will help in taking a perfect action by the company.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Skill Set Used: </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><b> Topics: </b> Machine Learning, Natural Language Processing, Sentiment analysis, Networking</p>\n",
    "<p><b> Machine Learning Alagorithms: </b> Naive Bayes algorithm, Multinomial Naive Bayes, Bernoulli Naive Bayes, Logistic Regression, Linear SVC</p>\n",
    "<p><b> Networking:</b> Basic Protocals(SMTP, TLS), Server-connection-indentification </p>\n",
    "<p><b> Robustness: </b>Ranking method</p>\n",
    "<p><b> Natural Language Processing:</b> Parsing, chunking, Parts of Speech tagging, Named Entity Recognition</p>\n",
    "<p><b> Compute: </b> python</p>\n",
    "<p><b> Data Tools:</b>Jupyter Notebook, Spyder</p>\n",
    "<p><b> Version Control: </b> Git</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
